{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# special big thanks to the library for figuring this out!\n",
    "\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "import os.path as path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    return (df\n",
    "        .replace(r'^s*$', float('NaN'), regex = True)  # There were blank rows, so first I changed them to \"NaN\" values\n",
    "        .dropna(axis=0, how='all')  # Then dropped any row where all the values were NaN\n",
    "        .assign(street=df.Address.str.rsplit('\\n', n=1, expand=True)[0].str.rsplit('#', n=1, expand=True)[0].str.rsplit(',', expand=True)[0], # Get street data. Assumes # or comma or nothing\n",
    "                unit1=df.Address.str.rsplit('\\n', n=1, expand=True)[0].str.extract(r'(\\d+$)|(\\#\\w+$)')[1].str.strip('#'), # Get unit data\n",
    "                unit2=df.Address.str.rsplit('\\n', n=1, expand=True)[0].str.rsplit('#', n=1, expand=True)[1], # Get unit data\n",
    "                unit3=df.Address.str.rsplit('\\n', n=1, expand=True)[0].str.lower().str.rsplit('unit', n=1, expand=True)[1].str.strip().str.lstrip('#'), # Get unit data\n",
    "                unit4=df.Address.str.rsplit('\\n', n=1, expand=True)[0].str.lower().str.rsplit('apt', n=1, expand=True)[1].str.strip().str.lstrip('#'), # Get unit data\n",
    "                unit5=df.Address.str.rsplit('\\n', n=1, expand=True)[0].str.lower().str.rsplit('suite', n=1, expand=True)[1].str.strip().str.lstrip('#'), # Get unit data\n",
    "                city=df.Address.str.rsplit('\\n', n=1, expand=True)[1].str.split(',', n=1, expand=True)[0],\n",
    "                state=df.Address.str.rsplit('\\n', n=1, expand=True)[1].str.split(',', n=1, expand=True)[1].str.rsplit(' ', n=1, expand=True)[0].str.strip(),\n",
    "                zip=df.Address.str.rsplit('\\n', n=1, expand=True)[1].str.split(',', n=1, expand=True)[1].str.rsplit(' ', n=1, expand=True)[1].str.strip())\n",
    "    )\n",
    "\n",
    "def load_within_directory(directory_string):\n",
    "    \"\"\"\n",
    "    Takes in a string of where your csv is located in the repo folder and turns\n",
    "    into a dataframe using pandas read_csv\n",
    "\n",
    "    example directory string: '/data/raw_data/CIE/client_needs_table.csv'\n",
    "    :param directory_string: string containing file with directory desired\n",
    "    :return: a dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    temp_path = path.abspath(path.join(\"data\" ,\"../../..\")) # finds the parent directory\n",
    "\n",
    "    # concatenates with directory\n",
    "    return temp_path + directory_string\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "pdf = pdfplumber.open(load_within_directory('/data/raw_data/sheriff_evictions_2018_jan_2023/sdso_lockout_service_activity_details_jan_2018_jan_2023.pdf'))\n",
    "\n",
    "table_settings = {\n",
    "    \"explicit_vertical_lines\": [20, 100, 220, 370, 465, 525, 600],\n",
    "    \"explicit_horizontal_lines\": [761],\n",
    "    \"horizontal_strategy\": \"lines\",\n",
    "    \"intersection_x_tolerance\": 50,\n",
    "    \"join_tolerance\": 300\n",
    "}\n",
    "\n",
    "# Scrape PDF and put rows into a list\n",
    "row_data = []\n",
    "for x in range(len(pdf.pages)):\n",
    "    p = pdf.pages[x]\n",
    "    table = p.extract_table(table_settings)\n",
    "    for row in table:\n",
    "        row_data.append(row)\n",
    "\n",
    "# Create a pandas dataframe from the pdf data being stored in a list\n",
    "df = pd.DataFrame(row_data, columns=['File_Number', 'Occupants', 'Address', 'Restoration_Date', 'Time', 'Status'])\n",
    "\n",
    "# Clean the raw data from the pdf\n",
    "df2 = clean_data(df)\n",
    "\n",
    "# Combine all the unit columns into one\n",
    "df2.unit1.update(df2.unit2)\n",
    "df2.unit1.update(df2.unit3)\n",
    "df2.unit1.update(df2.unit4)\n",
    "df2.unit1.update(df2.unit5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "directory = load_within_directory('/data/raw_data/sheriff_evictions_2018_jan_2023/')\n",
    "directory = directory + 'evictions_library_export.csv'\n",
    "df2.to_csv(directory, index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# (df2\n",
    "#     .drop(columns=['unit2', 'unit3', 'unit4', 'unit5']) # Drop the extra unit columns since they're now combined\n",
    "#     .rename(columns={'unit1': 'unit'}) # Rename the combined unit1 column to unit\n",
    "#     .to_excel('evictions_cleaned.xlsx', index=False) # Save to Excel\n",
    "# )\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}