{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os.path as path # used for easily finding the csvs in other directories\n",
    "from PyPDF2 import PdfReader # used to read and extract text from PDFs\n",
    "\n",
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt, seaborn as sns\n",
    "from datetime import datetime\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "outputs": [],
   "source": [
    "# loading csv from local directory function\n",
    "def load_within_directory(directory_string):\n",
    "    \"\"\"\n",
    "    Takes in a string of where your csv is located in the repo folder and turns\n",
    "    into a dataframe using pandas read_csv\n",
    "\n",
    "    example directory string: '/data/raw_data/CIE/client_needs_table.csv'\n",
    "    :param directory_string: string containing file with directory desired\n",
    "    :return: a dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    temp_path = path.abspath(path.join(\"data\" ,\"../../..\")) # finds the parent directory\n",
    "\n",
    "    # concatenates with directory\n",
    "    return temp_path + directory_string\n",
    "\n",
    "# imported from library cleaning script\n",
    "def clean_data(df):\n",
    "    return (df\n",
    "        .replace(r'^s*$', float('NaN'), regex = True)  # There were blank rows, so first I changed them to \"NaN\" values\n",
    "        .dropna(axis=0, how='all')  # Then dropped any row where all the values were NaN\n",
    "        .assign(street=df.Address.str.rsplit('\\n', n=1, expand=True)[0].str.rsplit('#', n=1, expand=True)[0].str.rsplit(',', expand=True)[0], # Get street data. Assumes # or comma or nothing\n",
    "                unit1=df.Address.str.rsplit('\\n', n=1, expand=True)[0].str.extract(r'(\\d+$)|(\\#\\w+$)')[1].str.strip('#'), # Get unit data\n",
    "                unit2=df.Address.str.rsplit('\\n', n=1, expand=True)[0].str.rsplit('#', n=1, expand=True)[1], # Get unit data\n",
    "                unit3=df.Address.str.rsplit('\\n', n=1, expand=True)[0].str.lower().str.rsplit('unit', n=1, expand=True)[1].str.strip().str.lstrip('#'), # Get unit data\n",
    "                unit4=df.Address.str.rsplit('\\n', n=1, expand=True)[0].str.lower().str.rsplit('apt', n=1, expand=True)[1].str.strip().str.lstrip('#'), # Get unit data\n",
    "                unit5=df.Address.str.rsplit('\\n', n=1, expand=True)[0].str.lower().str.rsplit('suite', n=1, expand=True)[1].str.strip().str.lstrip('#'), # Get unit data\n",
    "                city=df.Address.str.rsplit('\\n', n=1, expand=True)[0].str.split(',', n=1, expand=True)[0],\n",
    "                state=df.Address.str.rsplit('\\n', n=1, expand=True)[0].str.split(',', n=1, expand=True)[1].str.rsplit(' ', n=1, expand=True)[0].str.strip(),\n",
    "                zip=df.Address.str.rsplit('\\n', n=1, expand=True)[0].str.split(',', n=1, expand=True)[1].str.rsplit(' ', n=1, expand=True)[1].str.strip())\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "outputs": [],
   "source": [
    "# loading sheriff pdf\n",
    "sheriff_pdf = PdfReader(load_within_directory('/data/raw_data/sheriff_evictions_2018_jan_2023/sdso_lockout_service_activity_details_jan_2018_jan_2023.pdf'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1184\n",
      "SAN DIEGO COUNTY SHERIFF'S OFFICE\n",
      "Eviction List\n",
      "TO 01/01/2018 01/31/2023\n",
      "Office Phone #: (619) 544-6401\n",
      "Address Status File Number Occupants Restoration Date Time\n",
      "Chula Vista\n",
      "2015250864 NOTFOUND Alex Luevano,All Unknown \n",
      "Occupants,Carlos \n",
      "Luevano1256 8th Street\n",
      "(Front and Back House)\n",
      "Imperial Beach, CA 9193204/21/2021  12:00 AM\n",
      "2015250883 CANCELLED Audra Souza, Anthony A . \n",
      "Souza464 E H St #503\n",
      "Chula Vista , CA 9191005/20/2020  12:00 AM\n",
      "2017253065 SERVED Maria Varela ,Luis Matheu \n",
      "III1754 Via Costina\n",
      "San Diego, CA 9217301/04/2018  12:00 AM\n",
      "2017253067 SERVED All Unknown \n",
      "Occupants, Charvella West1357 Burgundy Dr\n",
      "Chula Vista , CA 9191301/04/2018  12:00 AM\n",
      "2017253088 SERVED Hildelisa Ochoa,All \n",
      "Unknown Occupants918 Tenth St , #3\n",
      "Coronado, CA 9211801/03/2018  12:00 AM\n",
      "2017253090 SERVED All Unknown \n",
      "Occupants,Raphael \n",
      "Vazquez ,Vanessa Rachel \n",
      "Vazquez1746 Via Capri\n",
      "Chula Vista , CA 9191301/03/2018  12:00 AM\n",
      "2017253147 SERVED Rogel Trucking , \n",
      "LLC,Jeremias Rogel Jr,All \n",
      "Unknown Occupants10030 Marconi Dr Ste D\n",
      "San Diego, CA 9215401/09/2018  12:00 AM\n",
      "2017253248 SERVED All Unknown \n",
      "Occupants, Debra Ann \n",
      "Roberson2131 Palo Alto Dr , Unit #108\n",
      "Chula Vista , CA 9191401/24/2018  12:00 AM\n",
      "2017253299 SERVED Tameika Delaney ,All \n",
      "Unknown Occupants513 Park Way #12\n",
      "Chula Vista , CA 9191001/03/2018  12:00 AM\n",
      "2017253305 SERVED United LC Capital LLC a \n",
      "Ca.L.L. co. &amp; dba \n",
      "Love CultureWestfield Plaza Bonita\n",
      "3030 Plaza Bonita Rd Str #2280\n",
      "National City, CA 9195001/16/2018  12:00 AM\n",
      "2017253313 SERVED All Unknown \n",
      "Occupants,Marisabel \n",
      "Rangel,Jessica M Mullaly3916 Rene Dr\n",
      "San Diego, CA 9215401/03/2018  12:00 AM\n",
      "2017253314 CANCELLED Jessica Benavidos,All \n",
      "Unknown \n",
      "Occupants,Justin Landry2708 Lincoln Ct #2\n",
      "National City, CA 9195001/03/2018  12:00 AM\n",
      "Printed: 02/16/2023 Page 1 of 1184\n"
     ]
    }
   ],
   "source": [
    "# this cell is just to detail what will go on in the pdf\n",
    "# parsing loop\n",
    "\n",
    "# We need to check the length of pages in the file\n",
    "print(len(sheriff_pdf.pages)) # should be 1184\n",
    "\n",
    "# grab a specific page from the pdf file\n",
    "temp_page = sheriff_pdf.pages[0]\n",
    "\n",
    "# then extract the text from page and save it\n",
    "temp_text = temp_page.extract_text()\n",
    "print(temp_text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# SDSO PDF Extraction Tool\n",
    "## How this works\n",
    "* First, the PDF is read from the above code, luckily it is fairly light on images so it is no larger than 4 MB\n",
    "* Second, the loop below will scrape every single page of text and store it into a large object (like a big text file)\n",
    "    * This makes it easier to parse through and format into a dataframe\n",
    "* Third, we need to make a string parser loop which will store a dictionary of each \"column\" for the dataframe. This is a bit difficult as we will essentially need to figure out how many lines each \"row\" this takes up.\n",
    "    * The parser divides the giant text document into rows, then adds them to each respective column pieceewise\n",
    "* Each column will be merged into a single dataframe with the hopes that it will be much easier to analyse.\n",
    "---"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "outputs": [],
   "source": [
    "# making a string object to append to\n",
    "raw_pdf_string = \"\"\n",
    "\n",
    "for i in range(0, len(sheriff_pdf.pages)):\n",
    "    temp_page = sheriff_pdf.pages[i]\n",
    "    temp_text = temp_page.extract_text()\n",
    "    raw_pdf_string = raw_pdf_string + temp_text\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "So the format for each row should be:\n",
    "\n",
    "Chula Vista\n",
    "\n",
    "2015250864 NOTFOUND Alex Luevano,All Unknown\n",
    "\n",
    "Occupants,Carlos\n",
    "\n",
    "Luevano1256 8th Street\n",
    "\n",
    "(Front and Back House)\n",
    "\n",
    "Imperial Beach, CA 9193204/21/2021  12:00 AM\n",
    "\n",
    "* First we need to remove the initial title\n",
    "* Then try chunking them out into a proper row or column header/unneeded info\n",
    "\n",
    "---"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "outputs": [],
   "source": [
    "# converting lines into a list to index through\n",
    "raw_pdf_list = raw_pdf_string.splitlines()\n",
    "del raw_pdf_list[0:6] # deleting the police header to make it easier to process\n",
    "# very odd \"Chula Vista\" entry? Was this a mistake?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "outputs": [],
   "source": [
    "# okay, so we can see that the items starting with \"Address\" should be removed\n",
    "# any lines beginning with \"Printed:\" also must be removed\n",
    "# look at this for an example, 'Printed:' in raw_pdf_list[46]\n",
    "for i in reversed(range(len(raw_pdf_list))): # reversing through list as dropping indeces causes issues\n",
    "    if 'Printed:' in raw_pdf_list[i]:\n",
    "        del raw_pdf_list[i]\n",
    "# now we have no more headings or needless info other than rows!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "outputs": [],
   "source": [
    "# next, we need to split up all of these into row cells\n",
    "# the best way would be to look for the file no. and end at the time\n",
    "# raw_pdf_list[0][0:2] to check for 20 or 30\n",
    "# raw_pdf_list[4][-2::1] # to check for AM or PM\n",
    "raw_rows = []\n",
    "temp_string = \"\"\n",
    "\n",
    "for string in raw_pdf_list:\n",
    "    temp_string = temp_string + string\n",
    "    if (string[-2::1] == \"AM\") or (string[-2::1] == \"PM\"):\n",
    "        raw_rows.append(temp_string)\n",
    "        temp_string = \"\"\n",
    "\n",
    "# also holy moly, is it just an error or laziness?\n",
    "# all the recorded Times as 12:00AM..."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "outputs": [],
   "source": [
    "# first lets get the case no.\n",
    "# easy enough, just strip the first 10 digits from the left\n",
    "case_no_list = []\n",
    "for i in range(0, len(raw_rows), 1):\n",
    "    case_no = raw_rows[i][0:10]\n",
    "    case_no_list.append(case_no)\n",
    "    raw_rows[i] = raw_rows[i].replace(case_no, '')\n",
    "    raw_rows[i] = raw_rows[i].lstrip()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "outputs": [],
   "source": [
    "# next for the case statuses\n",
    "status_list = []\n",
    "for i in range(0, len(raw_rows), 1):\n",
    "    all_words = raw_rows[i].split()\n",
    "    status = all_words[0]\n",
    "    del all_words[0]\n",
    "    raw_rows[i] = ' '.join(all_words)\n",
    "    status_list.append(status)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "outputs": [],
   "source": [
    "# next are the dates\n",
    "# raw_rows[0][-19::1] is the format\n",
    "date_list = []\n",
    "for i in range(0, len(raw_rows), 1):\n",
    "    date = raw_rows[i][-19::1]\n",
    "    date_list.append(date)\n",
    "    raw_rows[i] = raw_rows[i].replace(date, '')\n",
    "    raw_rows[i] = raw_rows[i].rstrip()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len sheriff_df 12799 len library_df 13926\n"
     ]
    }
   ],
   "source": [
    "# current progress, almost done, just need to split the\n",
    "# names and addresses. This Might be a little tricky...\n",
    "\n",
    "# For now, here is the current dataframe, we will instead add in the columns\n",
    "# cleaned from the library's method while also doing other forms of cleaning\n",
    "dataframe_dict = {\"File_Number\":case_no_list, \"Raw_Rows\":raw_rows, \"Restoration_Date\":date_list, \"Status\":status_list}\n",
    "sheriff_df = pd.DataFrame(dataframe_dict)\n",
    "\n",
    "# loading library df from EvictionPDF_altered script\n",
    "library_df = pd.read_csv(load_within_directory('/data/raw_data/sheriff_evictions_2018_jan_2023/evictions_library_export.csv'))\n",
    "\n",
    "\n",
    "# there are noticeably less rows than the other method\n",
    "# this could be from duplicates, so let's account for those\n",
    "\n",
    "# per each column\n",
    "sheriff_df = sheriff_df.drop_duplicates(subset = ['File_Number'], keep = 'first', ignore_index=True)\n",
    "\n",
    "# oddly less library, so let's account for them in that df too\n",
    "library_df = library_df.drop_duplicates(subset = ['File_Number'], keep = 'first', ignore_index=True)\n",
    "\n",
    "# let's check\n",
    "print(\"len sheriff_df\", len(sheriff_df), \"len library_df\",len(library_df))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "outputs": [],
   "source": [
    "# alright, so while the original best course of action would be to merge\n",
    "# pdf plumber's extraction uses a very threshold-based extraction where\n",
    "# we do not have exact benchmarks to judge how well it is identifying the table contents\n",
    "# in an effort to be as precise as possible, we will have to go with sheriff_df and use a string\n",
    "# cleaning method instead (using regex)\n",
    "\n",
    "# making an empty column to add values\n",
    "sheriff_df[\"Address\"] = \"\"\n",
    "\n",
    "# loop through all raw rows with names and addresses. Almost all but 1 addresses\n",
    "for i in range(0, len(sheriff_df)):\n",
    "    temp_string = sheriff_df[\"Raw_Rows\"][i] # pulling the string\n",
    "    numeric_found = re.search(r\"\\d\", temp_string) # locating the first instance of a numeric\n",
    "    if numeric_found: # if we found the address\n",
    "        address_string = temp_string[numeric_found.start():len(temp_string)] # slice it out\n",
    "        sheriff_df[\"Address\"][i] = address_string # add it to the row under the new column\n",
    "        sheriff_df[\"Raw_Rows\"][i] = sheriff_df[\"Raw_Rows\"][i].replace(address_string, '') # and remove it from the raw_row\n",
    "    else: # if not\n",
    "        sheriff_df[\"Address\"][i] = \"Warning! Address Now Found!\" # add a warning, do not mess with the raw data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "outputs": [],
   "source": [
    "# couple things left, splitting Time off from the date so that we can feed\n",
    "# this df into the library function\n",
    "\n",
    "sheriff_df[\"Time\"] = \"\"\n",
    "\n",
    "for i in range(0, len(sheriff_df)):\n",
    "    time = sheriff_df[\"Restoration_Date\"][i][-8::1]\n",
    "    sheriff_df[\"Time\"][i] = time\n",
    "    sheriff_df[\"Restoration_Date\"][i] = sheriff_df[\"Restoration_Date\"][i].replace(time, '')\n",
    "    sheriff_df[\"Restoration_Date\"][i] = sheriff_df[\"Restoration_Date\"][i].rstrip()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "outputs": [],
   "source": [
    "# to use the library script, we need our columns to match this order\n",
    "# File_Number\tOccupants\tAddress\tRestoration_Date\tTime\tStatus\n",
    "# so re_ordering and renaming the columns\n",
    "sheriff_df = sheriff_df.rename({'Raw_Rows':'Occupants'}, axis='columns') # renaming raw_rows to Occupants\n",
    "\n",
    "# reordering\n",
    "sheriff_df = sheriff_df[['File_Number', 'Occupants', 'Address', 'Restoration_Date', 'Time', 'Status']]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "outputs": [],
   "source": [
    "# cleaning using the library script\n",
    "# NOTE: due to some of the digits not being fully seperable from the aaddress, this means\n",
    "# the script is unable to fully divide, so for now it will just make mistakes\n",
    "# this can be fixed, will just take some more time.\n",
    "\n",
    "sheriff_df_library_cleaned = clean_data(sheriff_df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "outputs": [],
   "source": [
    "# exporting all dataframes\n",
    "\n",
    "# let's do the library_hybrid first (not recommended for use)\n",
    "directory = load_within_directory('/data/raw_data/sheriff_evictions_2018_jan_2023/')\n",
    "directory = directory + 'cleaned_evictions_library_hybrid_export.csv'\n",
    "sheriff_df_library_cleaned.to_csv(directory, index=False)\n",
    "\n",
    "# next our own version (better cleaned)\n",
    "directory = load_within_directory('/data/raw_data/sheriff_evictions_2018_jan_2023/')\n",
    "directory = directory + 'cleaned_evictions_export.csv'\n",
    "sheriff_df.to_csv(directory, index=False)\n",
    "\n",
    "# and finally the library df with removed duplicates\n",
    "directory = load_within_directory('/data/raw_data/sheriff_evictions_2018_jan_2023/')\n",
    "directory = directory + 'evictions_library_no_duplicates_export.csv'\n",
    "library_df.to_csv(directory, index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "outputs": [],
   "source": [
    "# sheriff_df.loc[sheriff_df['File_Number'] == \"2019453282\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}